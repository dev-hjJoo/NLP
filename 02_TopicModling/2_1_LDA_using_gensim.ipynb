{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim을 이용한 LDA 실습 코드\n",
    "* 출처: [딥 러닝을 이용한 자연어 처리 입문 / 19-02 잠재 디리클레 할당 (Latent Dirichlet Allocation)](https://wikidocs.net/30708)\n",
    "* 본 코드는 아래와 같은 순서로 구성되어 있습니다.\n",
    "    1. 뉴스그룹 데이터 불러오기 및 전처리 (LSA 코드와 동일)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 데이터 로드 & 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "'''1. load data'''\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "'''2. preprocessing'''\n",
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "'''3. remove stopwords'''\n",
    "# 불용어 데이터셋이 없을 경우 아래 코드의 주석을 해제하고 실행하기\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') # NLTK로부터 불용어 불러오기\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased., disagree, s...\n",
       "1    [yeah,, expect, people, read, faq,, etc., actu...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal,,...\n",
       "4    [well,, change, scoring, playoff, pool., unfor...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련용 데이터 확인\n",
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 2), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1)]\n"
     ]
    }
   ],
   "source": [
    "# 각각의 단어들을 (단어의 정수 인코딩 값, 해당 단어의 빈도수) 형태로 저장\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye-bye,\n"
     ]
    }
   ],
   "source": [
    "# 66이라는 정수 인코딩 된 숫자가 인코딩 전에는 어떤 단어였는지 확인하기\n",
    "print(dictionary[66])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181856"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 학습된 단어의 개수 확인\n",
    "len(dictionary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. LDA 모델 훈련 (Gensim 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.014*\"period\" + 0.011*\"----\" + 0.010*\"power\" + 0.009*\"---------------\"')\n",
      "(1, '0.015*\"team\" + 0.015*\"game\" + 0.010*\"games\" + 0.010*\"play\"')\n",
      "(2, '0.016*\"anyone\" + 0.014*\"would\" + 0.014*\"please\" + 0.013*\"know\"')\n",
      "(3, '0.003*\"hong\" + 0.002*\"kong\" + 0.002*\"championships\" + 0.002*\"answer:\"')\n",
      "(4, '0.007*\"would\" + 0.006*\"people\" + 0.005*\"many\" + 0.004*\"government\"')\n",
      "(5, '0.012*\"would\" + 0.010*\"people\" + 0.009*\"think\" + 0.007*\"like\"')\n",
      "(6, '0.013*\"entries\" + 0.004*\"hawks\" + 0.004*\"cubs\" + 0.003*\"caps\"')\n",
      "(7, '0.005*\"jumper\" + 0.004*\"slave\" + 0.003*\"jumpers\" + 0.002*\"esdi\"')\n",
      "(8, '0.024*\"armenian\" + 0.018*\"armenians\" + 0.018*\"turkish\" + 0.008*\"turkey\"')\n",
      "(9, '0.008*\"chicago\" + 0.006*\"boston\" + 0.006*\"detroit\" + 0.006*\"minnesota\"')\n",
      "(10, '0.007*\"radar\" + 0.004*\"candida\" + 0.004*\"stealth\" + 0.003*\"gateway\"')\n",
      "(11, '0.005*\"*******\" + 0.004*\"alcohol\" + 0.003*\"present.\" + 0.003*\"(205)\"')\n",
      "(12, '0.003*\"twin\" + 0.003*\"patterns\" + 0.002*\"------------------------------------------------------------------------\" + 0.002*\"objective.\"')\n",
      "(13, '0.010*\"like\" + 0.009*\"would\" + 0.008*\"good\" + 0.007*\"much\"')\n",
      "(14, '0.008*\"gordon\" + 0.008*\"----------------------------------------------------------------------------\" + 0.008*\"banks\" + 0.007*\"surrender\"')\n",
      "(15, '0.025*\"drive\" + 0.015*\"disk\" + 0.012*\"scsi\" + 0.011*\"hard\"')\n",
      "(16, '0.004*\"\"clipper\" + 0.004*\"chip\"\" + 0.003*\"baud\" + 0.003*\"0.333\"')\n",
      "(17, '0.020*\"israel\" + 0.018*\"israeli\" + 0.017*\"jews\" + 0.011*\"arab\"')\n",
      "(18, '0.011*\"space\" + 0.008*\"information\" + 0.006*\"data\" + 0.006*\"encryption\"')\n",
      "(19, '0.016*\"file\" + 0.010*\"program\" + 0.009*\"available\" + 0.008*\"window\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 20 # 20개의 토픽, k=20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "# 한 토픽당 출력할 단어 개수 지정\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "\n",
    "# 맨 앞에 있는 숫자는 토픽 번호로, 현재 토픽을 20개로 제한하였기 때문에 0부터 19까지 할당\n",
    "# 각 단어 앞에 붙은 수치는 해당 토픽에 대한 기여도\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. 문서별 토픽 분포 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문서의 topic 비율은 [(4, 0.34551287), (5, 0.3215774), (17, 0.31999916)]\n",
      "1 번째 문서의 topic 비율은 [(2, 0.14627048), (4, 0.25416204), (5, 0.57923603)]\n",
      "2 번째 문서의 topic 비율은 [(4, 0.16899686), (5, 0.5783655), (15, 0.063890666), (17, 0.17515764)]\n",
      "3 번째 문서의 topic 비율은 [(2, 0.3231347), (4, 0.1818687), (5, 0.17374781), (13, 0.1679064), (15, 0.015675435), (18, 0.12761773)]\n",
      "4 번째 문서의 topic 비율은 [(1, 0.25283885), (2, 0.05036209), (5, 0.30843422), (6, 0.2531329), (13, 0.07076233), (14, 0.03847841)]\n"
     ]
    }
   ],
   "source": [
    "# 상위 5개 문서에 대해 토픽 분포 확인\n",
    "# (숫자, 확률)은 토픽 번호와 해당 토픽이 문서에서 차지하는 분포도를 의미함\n",
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i==5:\n",
    "        break\n",
    "    print(i,'번째 문서의 topic 비율은',topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
    "\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
    "                topic_table = pd.concat([topic_table, pd.DataFrame([int(topic_num), round(prop_topic,4), topic_list]).T], ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 비중이 높은 토픽</th>\n",
       "      <th>가장 높은 토픽의 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>[(4, 0.3455353), (5, 0.32155597), (17, 0.31999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5792</td>\n",
       "      <td>[(2, 0.14627257), (4, 0.2541641), (5, 0.5792319)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5784</td>\n",
       "      <td>[(4, 0.16900979), (5, 0.57835346), (15, 0.0638...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>[(2, 0.3231304), (4, 0.18179756), (5, 0.173639...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3084</td>\n",
       "      <td>[(1, 0.25284672), (2, 0.0503644), (5, 0.308386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5935</td>\n",
       "      <td>[(5, 0.5935031), (10, 0.36552846)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3305</td>\n",
       "      <td>[(2, 0.17577219), (5, 0.3304737), (10, 0.15528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3728</td>\n",
       "      <td>[(4, 0.08824316), (5, 0.37281448), (13, 0.2845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5445</td>\n",
       "      <td>[(5, 0.5444543), (13, 0.14496103), (16, 0.2862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>[(2, 0.11892111), (4, 0.11527285), (5, 0.15554...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   문서 번호 가장 비중이 높은 토픽 가장 높은 토픽의 비중   \n",
       "0      0            4       0.3455  \\\n",
       "1      1            5       0.5792   \n",
       "2      2            5       0.5784   \n",
       "3      3            2       0.3231   \n",
       "4      4            5       0.3084   \n",
       "5      5            5       0.5935   \n",
       "6      6            5       0.3305   \n",
       "7      7            5       0.3728   \n",
       "8      8            5       0.5445   \n",
       "9      9           13       0.5004   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(4, 0.3455353), (5, 0.32155597), (17, 0.31999...  \n",
       "1  [(2, 0.14627257), (4, 0.2541641), (5, 0.5792319)]  \n",
       "2  [(4, 0.16900979), (5, 0.57835346), (15, 0.0638...  \n",
       "3  [(2, 0.3231304), (4, 0.18179756), (5, 0.173639...  \n",
       "4  [(1, 0.25284672), (2, 0.0503644), (5, 0.308386...  \n",
       "5                 [(5, 0.5935031), (10, 0.36552846)]  \n",
       "6  [(2, 0.17577219), (5, 0.3304737), (10, 0.15528...  \n",
       "7  [(4, 0.08824316), (5, 0.37281448), (13, 0.2845...  \n",
       "8  [(5, 0.5444543), (13, 0.14496103), (16, 0.2862...  \n",
       "9  [(2, 0.11892111), (4, 0.11527285), (5, 0.15554...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
    "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topictable[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoBERT",
   "language": "python",
   "name": "kobert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
