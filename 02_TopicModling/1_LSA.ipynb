{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA 이해를 위한 실습 코드\n",
    "* 출처: [딥 러닝을 이용한 자연어 처리 입문 / 19-01 잠재 의미 분석 (Latent Semantic Analysis)](https://wikidocs.net/24949)\n",
    "* 본 코드는 아래와 같은 순서로 구성되어 있습니다.\n",
    "    1. Full SVD 생성\n",
    "    2. 1에서 생성된 SVD를 기반으로 Truncated SVD 생성 (t=2)\n",
    "    3. Truncated SVD의 의미 정리\n",
    "    4. [응용] Scikit-learn 라이브러리의 Twenty Newsgroups 데이터를 이용하여 LSA 실습 (TF-IDF 행렬 생성)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Full SVD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![스크린샷 2023-06-13 143328](https://github.com/dev-hjJoo/NLP/assets/33647482/c793cb17-35dc-4631-a569-77f43dfcdd2f)\n",
    "* 위 이미지와 같은 DTM을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM의 크기(shape) : (4, 9)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "print('DTM의 크기(shape) :', np.shape(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 총 9개의 단어에 대한 문서 4개의 벡터를 생성하였으므로, Shape은 (4, 9)가 된다.\n",
    "* 이에 대해서 Full SVD를 수행해본다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full SVD를 이해하기 위한 용어\n",
    "1. 직교 행렬(Orthogonal matrix): 자신과 전치 행렬의 곱(반대도 성립)의 결과가 단위 행렬이되는 행렬로, 직교 행렬의 역행렬은 전치행렬임.\n",
    "2. 전치 행렬(Transpose matrix): 원래의 행렬에서 행과 열을 변환한 행렬\n",
    "3. 단위 행렬(Identify matrix): 주대각선의 원소가 모두 1이며 원소는 모두 0인 **정사각** 행렬\n",
    "4. 대각 행렬(Diagonal matrix): 주대각선을 제외한 곳의 원소가 모두 0인 행렬 (정사각 행렬이 아닐 수도 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n",
      "행렬 U의 크기(shape) : (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# U: 직교 행렬 (4*4)\n",
    "# s: 특이값 벡터\n",
    "# VT: V의 전치 행렬\n",
    "U, s, VT = np.linalg.svd(A, full_matrices = True)\n",
    "print('행렬 U :')\n",
    "print(U.round(2))\n",
    "print('행렬 U의 크기(shape) :',np.shape(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특이값 벡터 :\n",
      "[2.69 2.05 1.73 0.77]\n",
      "특이값 벡터의 크기(shape) : (4,)\n"
     ]
    }
   ],
   "source": [
    "print('특이값 벡터 :')\n",
    "print(s.round(2))\n",
    "print('특이값 벡터의 크기(shape) :',np.shape(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대각 행렬 S :\n",
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n",
      "대각 행렬의 크기(shape) :\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "# 특이값 s는 대각행렬의 값만 추출한 것으로, 대각행렬을 확인하기 위해서는 크기가 알맞은 행렬 형태를 생성하여야 함\n",
    "# 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성\n",
    "S = np.zeros((4, 9))\n",
    "\n",
    "# 특이값을 대각행렬에 삽입\n",
    "S[:4, :4] = np.diag(s)\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))\n",
    "\n",
    "print('대각 행렬의 크기(shape) :')\n",
    "print(np.shape(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n",
      "직교 행렬 VT의 크기(shape) :\n",
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "# V의 전치 행렬 VT 생성\n",
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))\n",
    "\n",
    "print('직교 행렬 VT의 크기(shape) :')\n",
    "print(np.shape(VT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 행렬 A:\n",
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "U, S, VT를 곱한 행렬:\n",
      "[[ 0.  0.  0.  1.  0.  1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1. -0.  1. -0. -0.]\n",
      " [ 0.  1.  1. -0.  2. -0. -0.  0.  0.]\n",
      " [ 1. -0.  0.  0. -0.  0. -0.  1.  1.]]\n",
      "두 행렬이 동일한 지 확인: True\n"
     ]
    }
   ],
   "source": [
    "# U, S, VT를 곱하면 기존의 행렬 A가 나와야 함\n",
    "# Numpy의 allclose() 메소드를 통해 행렬이 동일한지 확인\n",
    "\n",
    "print('기존 행렬 A:')\n",
    "print(A)\n",
    "print('U, S, VT를 곱한 행렬:')\n",
    "print(np.dot(np.dot(U, S), VT).round(2))    # 소수점 아래 두자리만 출력\n",
    "print('두 행렬이 동일한 지 확인:', np.allclose(A, np.dot(np.dot(U, S), VT).round(2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Truncated SVD 생성\n",
    "* 축소할 차원 수(토픽 수, t)는 2로 설정\n",
    "* 여기서 차원 수는 상위 t개를 추출한다는 의미이며, t가 작을수록 원본 행렬과 차이가 커진다. (많이 축소시키기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대각 행렬 S :\n",
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "# 특이값 상위 2개만 보존\n",
    "S = S[:2,:2]\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75]\n",
      " [-0.51  0.44]\n",
      " [-0.83 -0.49]\n",
      " [-0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 직교 행렬 U에 대해서도 2개의 열만 남기고 제거\n",
    "U = U[:,:2]\n",
    "print('행렬 U :')\n",
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# VT에 대해서도 2개의 행만 남기고 제거 (V 관점에서 2개의 열만 남기고 제거한 것과 동일)\n",
    "VT = VT[:2,:]\n",
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 행렬 A:\n",
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "차원축소 후 U, S, VT를 곱한 행렬:\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 축소된 행렬에 대하여 U, S, VT를 곱하면 기존의 A와 다른 결과가 나옴 (값이 손실되었기 때문)\n",
    "# 이렇게 나온 결과를 기존의 행렬 A와 비교\n",
    "A_prime = np.dot(np.dot(U,S), VT)\n",
    "print('기존 행렬 A:')\n",
    "print(A)\n",
    "print('차원축소 후 U, S, VT를 곱한 행렬:')\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Truncated SVD의 의미 정리\n",
    "* 기존 행렬 A의 값과 유사한 결과가 나오지만, 제대로 복구되지 않은 부분도 존재함.\n",
    "#### 축소된 직교행렬 U 크기의 의미\n",
    "* 축소된 U의 크기는 4\\*2이며, 이는 문서의개수\\*토픽의 개수를 의미한다.\n",
    "* 즉, 총 4개의 문서에 대하여 단어 전체(9개)를 보지 않고 제한된 토픽(상위 2개)만 표현한 것이다.\n",
    "* 제대로 복구되지 않은 것은 상위 n개에 속하지 않는 토픽으로 제거된 것으로 보아도 무관하다.\n",
    "\n",
    "```최종적으로 Truncated SVD를 이용하여 데이터 정보를 압축하면서도 기존 행렬 A와 근사한 값을 가질 수 있음.```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Twenty Newsgroups 데이터를 이용한 LSA 실습\n",
    "* LSA를 사용하여 문서의 수를 원하는 토픽의 수로 압축한 뒤, 각 토픽당 가장 중요한 단어5개를 출력한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1. 뉴스 그룹 데이터에 대한 이해\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('샘플의 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련용 샘플 출력\n",
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 뉴스그룹 데이터에는 특수문자가 포함된 다수의 영어 문장이 포함되어 있음\n",
    "* 위와 같은 형태의 샘플이 총 11,314개 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# 뉴스 그룹 데이터의 기존 카테고리 출력\n",
    "print(dataset.target_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah, expect people read faq, etc. actually accept hard atheism? need little leap faith, jimmy. your logic runs steam! jim, sorry can't pity you, jim. sorry that have these feelings denial about faith need well, just pretend that will happily ever after anyway. maybe start newsgroup, alt.atheist.hard, won't bummin' much? bye-bye, jim. don't forget your flintstone's chewables! bake timmons,\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제 후 텍스트 확인\n",
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\etoil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 데이터셋 다운로드\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리\n",
    "stop_words = stopwords.words('english') # NLTK로부터 불용어 불러오기\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah,', 'expect', 'people', 'read', 'faq,', 'etc.', 'actually', 'accept', 'hard', 'atheism?', 'need', 'little', 'leap', 'faith,', 'jimmy.', 'logic', 'runs', 'steam!', 'jim,', 'sorry', \"can't\", 'pity', 'you,', 'jim.', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well,', 'pretend', 'happily', 'ever', 'anyway.', 'maybe', 'start', 'newsgroup,', 'alt.atheist.hard,', \"bummin'\", 'much?', 'bye-bye,', 'jim.', 'forget', \"flintstone's\", 'chewables!', 'bake', 'timmons,']\n"
     ]
    }
   ],
   "source": [
    "# 불용어(your, about, just, that, will, after, ...) 제거된 텍스트 확인\n",
    "print(tokenized_doc[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3. TF-IDF 행렬 만들기\n",
    "* Scikit-learn의 TfidfVectorizer 사용\n",
    "* 단어는 1,000개의 단어로 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 행렬의 크기 : (11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, # 상위 1,000개의 단어를 보존 \n",
    "max_df = 0.5, smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "\n",
    "# TF-IDF 행렬의 크기 확인\n",
    "print('TF-IDF 행렬의 크기 :',X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-4. Topic modeling\n",
    "* Scikit-learn의 TruncatedSVD 메소드 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존의 뉴스그룹 데이터가 가진 카테고리 수가 20개이므로, n_components는 20으로 설정\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전치 행렬 VT(=svd_model.components_)의 크기 출력: 토픽의 수 \\* 단어의 수\n",
    "np.shape(svd_model.components_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 아래 코드 실행할 때\n",
    "    * sklearn의 버전이 0.24.x 이하인 경우 **get_feature_names()** 사용\n",
    "    * sklearn의 버전이 1.0.x 이상인 경우 **get_feature_names_out()** 사용\n",
    "    * 참고 링크: [[stack overflow] AttributeError: 'TfidfVectorizer' object has no attribute 'get_feature_names_out'](https://stackoverflow.com/questions/70215049/attributeerror-tfidfvectorizer-object-has-no-attribute-get-feature-names-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('just', 0.20273), ('don', 0.19949), ('like', 0.19534), ('know', 0.18819), ('people', 0.17851)]\n",
      "Topic 2: [('thanks', 0.31973), ('windows', 0.27738), ('card', 0.17369), ('drive', 0.15927), ('mail', 0.1488)]\n",
      "Topic 3: [('game', 0.31991), ('team', 0.27972), ('year', 0.26585), ('games', 0.20684), ('drive', 0.17141)]\n",
      "Topic 4: [('edu', 0.42972), ('thanks', 0.24949), ('mail', 0.17243), ('game', 0.12812), ('team', 0.12645)]\n",
      "Topic 5: [('know', 0.41918), ('does', 0.305), ('thanks', 0.26133), ('don', 0.21061), ('just', 0.19501)]\n",
      "Topic 6: [('drive', 0.45668), ('edu', 0.21787), ('thanks', 0.18804), ('scsi', 0.1573), ('drives', 0.12178)]\n",
      "Topic 7: [('just', 0.56807), ('edu', 0.43053), ('don', 0.22492), ('like', 0.19983), ('soon', 0.09527)]\n",
      "Topic 8: [('chip', 0.2163), ('government', 0.2001), ('encryption', 0.14658), ('like', 0.14639), ('clipper', 0.14265)]\n",
      "Topic 9: [('don', 0.32188), ('know', 0.31942), ('edu', 0.28677), ('does', 0.26522), ('think', 0.19896)]\n",
      "Topic 10: [('does', 0.47651), ('card', 0.29892), ('just', 0.23942), ('video', 0.15102), ('like', 0.12375)]\n",
      "Topic 11: [('just', 0.41493), ('drive', 0.23901), ('does', 0.20644), ('file', 0.15788), ('game', 0.14789)]\n",
      "Topic 12: [('like', 0.52875), ('drive', 0.19739), ('does', 0.15354), ('file', 0.12715), ('time', 0.09433)]\n",
      "Topic 13: [('like', 0.48947), ('people', 0.40204), ('windows', 0.17144), ('israel', 0.12438), ('card', 0.12266)]\n",
      "Topic 14: [('think', 0.36298), ('just', 0.18776), ('com', 0.16087), ('like', 0.14128), ('chip', 0.12567)]\n",
      "Topic 15: [('know', 0.38843), ('00', 0.26278), ('don', 0.24262), ('sale', 0.15833), ('file', 0.14316)]\n",
      "Topic 16: [('good', 0.52031), ('people', 0.24556), ('windows', 0.23767), ('file', 0.17392), ('does', 0.16211)]\n",
      "Topic 17: [('think', 0.46225), ('space', 0.38068), ('does', 0.20763), ('israel', 0.1535), ('nasa', 0.13529)]\n",
      "Topic 18: [('com', 0.53463), ('ve', 0.44137), ('does', 0.25038), ('people', 0.14719), ('think', 0.12949)]\n",
      "Topic 19: [('space', 0.35337), ('ve', 0.25933), ('card', 0.16633), ('people', 0.16342), ('know', 0.12589)]\n",
      "Topic 20: [('ve', 0.44599), ('00', 0.24318), ('people', 0.20333), ('window', 0.15806), ('does', 0.15766)]\n"
     ]
    }
   ],
   "source": [
    "# 20개의 행에 대하여 1000개의 열 중 가장 값이 큰 5개 출력\n",
    "terms = vectorizer.get_feature_names_out() # 단어 집합. 1,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoBERT",
   "language": "python",
   "name": "kobert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
